{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53319c76-6c44-4f13-97f8-690afe91ebd2",
   "metadata": {},
   "source": [
    "Given our dataset with customer transactions, considering the characteristics and typical usage of various algorithms:\r\n",
    "\r\n",
    "### 1. **SMOTE (Synthetic Minority Over-sampling Technique)**\r\n",
    "SMOTE is best for scenarios where you're dealing with imbalanced classes, particularly in classification problems. It generates synthetic samples based on the nearest neighbors of minority class samples. In the context of your dataset, which appears to be a transactional dataset with various features, SMOTE might not be the most straightforward approach since it's usually applied to target variables in classification problems. However, if you're interested in synthetic expansion and have categorical targets, it could be adapted.\r\n",
    "\r\n",
    "### 2. **Data Augmentation**\r\n",
    "Data augmentation methods like adding noise or jittering are useful when you want to introduce slight variations in the dataset without fundamentally changing it. For your transaction dataset, you could add noise to numerical features or create synthetic variations in descriptions or quantities. This could be useful for expanding the dataset in a more realistic way.\r\n",
    "\r\n",
    "### 3. **Bootstrapping**\r\n",
    "Bootstrapping involves resampling your existing data with replacement. This method is simple and effective for increasing the size of your dataset and maintaining its original characteristics. It's particularly useful if your data is not imbalanced and you want to maintain the original distribution.\r\n",
    "\r\n",
    "### 4. **Data Duplication**\r\n",
    "Simply duplicating your dataset multiple times can also increase its size, although it may not introduce new information or variability. This method is straightforward but might not add significant value if diversity is required in the augmented dataset.\r\n",
    "\r\n",
    "### Recommended Approach\r\n",
    "For your dataset, a combination of **Bootstrapping** and **Data Augmentation** could be most effective:\r\n",
    "\r\n",
    "1. **Bootstrapping**: To directly increase the size of your dataset by resampling.\r\n",
    "2. **Data Augmentation**: Add noise or create synthetic variations to en used in combination with other methods for additional expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f14b31e8-bbb3-4740-a94c-74290d342ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6.220121</td>\n",
       "      <td>1/12/10 8:26</td>\n",
       "      <td>0.671104</td>\n",
       "      <td>17841.767219</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>8.269406</td>\n",
       "      <td>1/12/10 8:26</td>\n",
       "      <td>1.896648</td>\n",
       "      <td>17852.202819</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>12.616279</td>\n",
       "      <td>1/12/10 8:26</td>\n",
       "      <td>2.863311</td>\n",
       "      <td>17836.108498</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>3.039003</td>\n",
       "      <td>1/12/10 8:26</td>\n",
       "      <td>2.232953</td>\n",
       "      <td>17840.941535</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>2.261968</td>\n",
       "      <td>1/12/10 8:26</td>\n",
       "      <td>3.438669</td>\n",
       "      <td>17873.472343</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  InvoiceNo StockCode                          Description   Quantity  \\\n",
       "0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER   6.220121   \n",
       "1    536365     71053                  WHITE METAL LANTERN   8.269406   \n",
       "2    536365    84406B       CREAM CUPID HEARTS COAT HANGER  12.616279   \n",
       "3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE   3.039003   \n",
       "4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.   2.261968   \n",
       "\n",
       "    InvoiceDate  UnitPrice    CustomerID         Country  \n",
       "0  1/12/10 8:26   0.671104  17841.767219  United Kingdom  \n",
       "1  1/12/10 8:26   1.896648  17852.202819  United Kingdom  \n",
       "2  1/12/10 8:26   2.863311  17836.108498  United Kingdom  \n",
       "3  1/12/10 8:26   2.232953  17840.941535  United Kingdom  \n",
       "4  1/12/10 8:26   3.438669  17873.472343  United Kingdom  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(r'E:\\DBDA_CDAC\\Projects\\CDAC_project\\RetailData.csv')\n",
    "\n",
    "# Method 1: Add Gaussian noise to numeric columns\n",
    "def add_noise(data, noise_level=0.01):\n",
    "    noisy_data = data.copy()\n",
    "    for col in data.select_dtypes(include=[np.number]).columns:\n",
    "        noise = noise_level * data[col].std() * np.random.randn(len(data))\n",
    "        noisy_data[col] += noise\n",
    "    return noisy_data\n",
    "\n",
    "# Method 2: Bootstrap sampling\n",
    "def bootstrap_data(data, target_size):\n",
    "    current_size = len(data)\n",
    "    multiplier = target_size // current_size\n",
    "    remainder = target_size % current_size\n",
    "    \n",
    "    # Replicate the dataset\n",
    "    larger_dataset = pd.concat([data] * multiplier, ignore_index=True)\n",
    "    \n",
    "    # Add some additional samples using bootstrapping if needed\n",
    "    if remainder > 0:\n",
    "        additional_samples = resample(data, n_samples=remainder, replace=True)\n",
    "        larger_dataset = pd.concat([larger_dataset, additional_samples], ignore_index=True)\n",
    "    \n",
    "    return larger_dataset\n",
    "\n",
    "# Augment the dataset\n",
    "augmented_data = add_noise(data)\n",
    "augmented_data = bootstrap_data(augmented_data, target_size=10000000)  # 1 crore rows\n",
    "\n",
    "# Save the augmented dataset\n",
    "# augmented_data.to_csv('augmented_dataset.csv', index=False)\n",
    "augmented_data.shape\n",
    "augmented_data.head()\n",
    "# augmented_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f29aec-d54a-4c90-bbc5-6ea0b004e3bc",
   "metadata": {},
   "source": [
    "\r\n",
    "### 1. **Loading the Dataset**\r\n",
    "```python\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn.utils import resample\r\n",
    "\r\n",
    "# Load your dataset\r\n",
    "data = pd.read_csv('your_dataset.csv')\r\n",
    "```\r\n",
    "- **`pandas`** and **`numpy`** are imported for data manipulation and numerical operations.\r\n",
    "- **`resample`** is imported from `sklearn.utils` to perform bootstrapping.\r\n",
    "- The dataset is loaded from a CSV file into a DataFrame named `data`.\r\n",
    "\r\n",
    "### 2. **Adding Gaussian Noise to Numeric Columns**\r\n",
    "```python\r\n",
    "def add_noise(data, noise_level=0.01):\r\n",
    "    noisy_data = data.copy()\r\n",
    "    for col in data.select_dtypes(include=[np.number]).columns:\r\n",
    "        noise = noise_level * data[col].std() * np.random.randn(len(data))\r\n",
    "        noisy_data[col] += noise\r\n",
    "    return noisy_data\r\n",
    "```\r\n",
    "- **`add_noise`** is a function that adds Gaussian noise to the numeric columns in your dataset.\r\n",
    "- **`data.copy()`** creates a copy of the original dataset to avoid modifying it directly.\r\n",
    "- **`select_dtypes(include=[np.number])`** selects only the numeric columns in the dataset.\r\n",
    "- **`np.random.randn(len(data))`** generates random values from a standard normal distribution for each row.\r\n",
    "- The noise is scaled by the standard deviation of each column (`data[col].std()`) and a **`noise_level`** parameter (set to 0.01 by default).\r\n",
    "- The noisy data is then returned.\r\n",
    "\r\n",
    "### 3. **Bootstrapping the Dataset**\r\n",
    "```python\r\n",
    "def bootstrap_data(data, target_size):\r\n",
    "    current_size = len(data)\r\n",
    "    multiplier = target_size // current_size\r\n",
    "    remainder = target_size % current_size\r\n",
    "    \r\n",
    "    # Replicate the dataset\r\n",
    "    larger_dataset = pd.concat([data] * multiplier, ignore_index=True)\r\n",
    "    \r\n",
    "    # Add some additional samples using bootstrapping if needed\r\n",
    "    if remainder > 0:\r\n",
    "        additional_samples = resample(data, n_samples=remainder, replace=True)\r\n",
    "        larger_dataset = pd.concat([larger_dataset, additional_samples], ignore_index=True)\r\n",
    "    \r\n",
    "    return larger_dataset\r\n",
    "```\r\n",
    "- **`bootstrap_data`** is a function that increases the dataset size to a specified `target_size`.\r\n",
    "- **`current_size`** is the current number of rows in the dataset.\r\n",
    "- **`multiplier`** calculates how many full copies of the dataset are needed to approach the target size.\r\n",
    "- **`remainder`** calculates how many additional rows are needed after replicating the dataset multiple times.\r\n",
    "- **`pd.concat([data] * multiplier, ignore_index=True)`** creates a larger dataset by replicating the original dataset `multiplier` times. `ignore_index=True` ensures that the index is reset.\r\n",
    "- If additional rows are needed, **`resample`** is used to perform bootstrapping on the dataset and generate `remainder` rows.\r\n",
    "- The final dataset is returned.\r\n",
    "\r\n",
    "### 4. **Combining Data Augmentation and Bootstrapping**\r\n",
    "```python\r\n",
    "# Augment the dataset\r\n",
    "augmented_data = add_noise(data)\r\n",
    "1ugmented_dat1 = bootstrap_data(augmented_data, target_size=20000000)  # 2 crore rows\r\n",
    "```\r\n",
    "- The dataset is first augmented by adding noise using the `add_noise` function.\r\n",
    "- Then, the augmented dataset is further expanded using the `bootstrap_data` function to reach a target size of 2 crore rows (20 million rows).\r\n",
    "\r\n",
    "### 5. **Saving the Augmented Dataset**\r\n",
    "```python\r\n",
    "# Save the augmented dataset\r\n",
    "augmented_data.to_csv('augmented_dataset.csv', index=False)\r\n",
    "```\r\n",
    "- Finally, the augmented dataset is saved to a new CSV file named `augmented_dataset.csv`.\r\n",
    "- **`index=False`** ensures that the row indices are not included in the CSV file.\r\n",
    "\r\n",
    "### Summary of the Process:\r\n",
    "1. **Data Augmentation**: Adds Gaussian noise to numeric columns to introduce variability.\r\n",
    "2. **Bootstrapping**: Replicates and resamples the dataset to increase its size significantly.\r\n",
    "3. **Final Dataset**: Combines both methods to creing some variability to make the data more robust for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fdb2ac9-18f8-471a-a3e0-20a83b1db93f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:/DBDA_CDAC/Projects/CDAC_project/augmented_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mE:/DBDA_CDAC/Projects/CDAC_project/augmented_dataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:/DBDA_CDAC/Projects/CDAC_project/augmented_dataset.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r'E:/DBDA_CDAC/Projects/CDAC_project/augmented_dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76de3d6-253d-4c68-a380-743a9114f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['InvoiceDate'].max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
